{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/fmottes/jax-morph/blob/eqx\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" style=\"width:250px;\"/>\n",
    "  </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization of stochastic simulations\n",
    "\n",
    "We now turn to the problem of optimizing a simulation that involves steps where some sort of sampling is involved. The sampling step is not differentiable in the classic sense, so it is not possible to propagate gradients through the simulation directly.\n",
    "\n",
    "In this notebook we will use some toy problems to illustrate the basic variants of the two main ideas used in general to overcome the non-differentiability of the sampling step:\n",
    "\n",
    "1. **Reparametrization**: The idea is to reparametrize the sampling step in a way that makes it differentiable, by \"decoupling\" the learnable parameters from the randomness. This usually involves a transformation of the random variable that depends on the learnable parameters, and that is differentiable with respect to them. Heuristically, though, it is fairly hard to make this second approach work in practice, especially in complex models like ours.\n",
    "\n",
    "2. **Score function**: The idea is to use the score function to estimate the gradient of the expectation of the function of interest. This idea is at the core of the REINFORCE algorithm, the one we adopt also for the optimizations carried out in the paper.\n",
    "\n",
    "\n",
    "In order to illustrate the problem and the two approaches mentioned above, we will explore two different cases of increasing complexity: sampling from a single **Bernoulli Random Variable** first, and then from a **Categorical Distribution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1715094556.808931       1 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as np\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "jax.config.update(\"jax_debug_nans\", True)\n",
    "\n",
    "key = jax.random.PRNGKey(0) # random number generator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bernoulli Random Variable\n",
    "\n",
    "A Bernoulli random variable $X$ is a discrete random variable that takes on two possible outcomes, usually denoted as 0 and 1. The probability mass function of $X$ is given by:\n",
    "\n",
    "$$\n",
    "P(X = k) = \n",
    "\\begin{cases} \n",
    "p & \\text{if } k = 1 \\\\\n",
    "1 - p & \\text{if } k = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $p$ is the probability of the event occurring (i.e., $X = 1$), and $1 - p$ is the probability of the event not occurring (i.e., $X = 0$).\n",
    "\n",
    "\n",
    "We define the RV Z as distributed as Bernoulli \n",
    "$$ Z \\sim \\text{Bernoulli}(p) $$\n",
    "and the outcome of our \"simulation\" will be a function of the value of Z. \n",
    "\n",
    "\n",
    "In order to avoid possible confusions deriving from 0 values, we also redefine the outcomes of Z as +1 and -1, instead of 1 and 0.\n",
    "\n",
    "$$\n",
    "P(X = k) = \n",
    "\\begin{cases} \n",
    "p & \\text{if } k = +1 \\\\\n",
    "1 - p & \\text{if } k = -1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In order to make things (slighty) less trivial, we will also suppose that the probability $p$ depends on a parameter $\\theta$ that we want to learn. In particular, we will suppose that $p = \\sigma(\\theta)$, where $\\sigma$ is the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SIMULATION STEPS\n",
    "\n",
    "# calculate p given theta\n",
    "p_fn = lambda theta: jax.nn.sigmoid(theta)\n",
    "\n",
    "\n",
    "# sample from Bernoulli distribution given p\n",
    "def sample_bernoulli(subkey, p):\n",
    "\n",
    "    u = jax.random.uniform(subkey)\n",
    "\n",
    "    # Â±1 encoding instead of [0,1]\n",
    "    return np.sign(p-u)\n",
    "\n",
    "\n",
    "# deterministic part of the \"simulation\"\n",
    "env_fn = lambda z: 2*z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTIRE SIMULATION\n",
    "\n",
    "def simulation(subkey, theta):\n",
    "\n",
    "    p = p_fn(theta) # calculate p\n",
    "    z = sample_bernoulli(subkey, p) # sample z\n",
    "    outcome = env_fn(z) # simulate outcome\n",
    "\n",
    "    return outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gradient Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try to calculate the linear sensitivity of the model outcome with respect to the input parameter $\\theta$.\n",
    "\n",
    "The analytic expression for the gradient is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{outcome}}{\\partial \\theta}  = \\frac{\\partial \\text{outcome}}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial p} \\cdot \\frac{\\partial p}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "But the derivative\n",
    "$$ \\frac{\\partial Z}{\\partial p} =\\ ?$$\n",
    "is not defined, since Z is a discrete random variable. In this case, JAX will set the derivative to 0 and the whole expression will be 0 as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation outcome:\t2.0\n",
      "Gradient wrt theta:\t0.0\n"
     ]
    }
   ],
   "source": [
    "theta = 1. # gives p = 0.73\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "o, g = jax.value_and_grad(simulation, argnums=1)(subkey, theta)\n",
    "\n",
    "print(f'Simulation outcome:\\t{o}')\n",
    "print(f'Gradient wrt theta:\\t{g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check that each other step is well defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p = 0.7310585786300049 \t gradient = 0.19661193324148185\n",
      "z = -1.0 \t gradient = 0.0\n",
      "outcome = -2.0 \t gradient = 2.0\n"
     ]
    }
   ],
   "source": [
    "print(f'p = {p_fn(theta)} \\t\\t gradient = {jax.grad(p_fn)(theta)}')\n",
    "\n",
    "print(f'z = {sample_bernoulli(subkey, .5)} \\t\\t gradient = {jax.grad(sample_bernoulli, argnums=1)(subkey, .5)}')\n",
    "\n",
    "print(f'outcome = {env_fn(-1.)} \\t\\t gradient = {jax.grad(env_fn)(-1.)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1: Straight-Through Estimator\n",
    "\n",
    "One first fix is to decide on an arbitrary value (usually 1) to be assigned to the ill-defined derivative, so as to allow gradients to flow past the problematic point. This is the idea behind the Straight-Through Estimator and its more mathematically refined versions.\n",
    "\n",
    "$$ \\frac{\\partial Z}{\\partial p} \\sim\\ 1$$\n",
    "\n",
    "We can easily do this in JAX by applying the following trick with `jax.lax.stop_gradient` (check the JAX docs for details).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ST_sample_bernoulli(subkey, p):\n",
    "\n",
    "    u = jax.random.uniform(subkey)\n",
    "    z = np.sign(p-u)\n",
    "\n",
    "    zero =  p - jax.lax.stop_gradient(p)\n",
    "\n",
    "    #in the gradient calculation only the dependence on p remains, so the grad of the function is 1!!\n",
    "    return zero + jax.lax.stop_gradient(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  SIMULATION with ST step\n",
    "###  Other steps are unchanged\n",
    "\n",
    "def ST_simulation(subkey, theta):\n",
    "\n",
    "    p = p_fn(theta) # calculate p\n",
    "    z = ST_sample_bernoulli(subkey, p) # sample z\n",
    "    outcome = env_fn(z) # simulate outcome\n",
    "\n",
    "    return outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation outcome:\t-2.0\n",
      "Gradient wrt theta:\t0.3932238664829637\n"
     ]
    }
   ],
   "source": [
    "theta = 1. # gives p = 0.73\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "o, g = jax.value_and_grad(ST_simulation, argnums=1)(subkey, theta)\n",
    "\n",
    "print(f'Simulation outcome:\\t{o}')\n",
    "print(f'Gradient wrt theta:\\t{g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the outcome of the simulation is stochastic, in order to get a better gradient we can get the expected gradient value over many simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average simulation outcome:\t0.8428428428428428\n",
      "Average gradient wrt theta:\t0.3932238664829637\n"
     ]
    }
   ],
   "source": [
    "theta = 1. # gives p = 0.73\n",
    "\n",
    "N_AVG = 1000\n",
    "\n",
    "key, *subkeys = jax.random.split(key, N_AVG)\n",
    "subkeys = np.asarray(subkeys)\n",
    "\n",
    "o, g = jax.vmap(jax.value_and_grad(ST_simulation, argnums=1), in_axes=(0,None))(subkeys, theta)\n",
    "\n",
    "\n",
    "print(f'Average simulation outcome:\\t{o.mean()}')\n",
    "print(f'Average gradient wrt theta:\\t{g.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this agrees with the previous calculation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 2: Score Function Estimator (REINFORCE)\n",
    "\n",
    "The second approach is to let go of all hope of calculating the gradient of a single simulation, and calculate the gradient of its **expected value** instead. It can be shown that the gradient of the expected value of a function of a random variable can be expressed in terms of the score function of the distribution of the random variable.\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta}\\ \\mathbb{E}_{\\theta} [f(Z)] = \\mathbb{E}_{\\theta}\\ [f(Z)\\ \\nabla_{\\theta} \\log P_{\\theta}(Z)]\n",
    "$$\n",
    "\n",
    "That is, we now only need that the probability distribution from which we sample Z (Bernoulli in this specific case) be differentiable with respect to our parameters.\n",
    "\n",
    "We can estimate the expectation numerically by running a lot of simulations and recording the outcomes and the log-probabilities of the outcomes. Then we can differentiate the log-probabilities instead of the simulations!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### log-probability of outcome\n",
    "logp_fn = lambda z, p: np.log(p*(z==1) + (1-p)*(z==-1))\n",
    "\n",
    "\n",
    "### Modify simulation to return logp too\n",
    "\n",
    "def simulation_logp(subkey, theta):\n",
    "\n",
    "    p = p_fn(theta) # calculate p\n",
    "    z = sample_bernoulli(subkey, p) # sample z\n",
    "    outcome = env_fn(z) # simulate outcome\n",
    "\n",
    "    return outcome, logp_fn(z, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the gradient calculation more coincise, we can define a **surrogate loss**, which does not have a meaning *per se* but whose gradient is the the one we actually want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surrogate_loss(subkey, theta):\n",
    "\n",
    "    outcome, logp = simulation_logp(subkey, theta)\n",
    "\n",
    "    return jax.lax.stop_gradient(outcome)*logp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation outcome:\t2.0\n",
      "surrogate_loss value\t-0.6265233750364456\n",
      "Gradient wrt theta:\t0.5378828427399902\n"
     ]
    }
   ],
   "source": [
    "theta = 1. # gives p = 0.73\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "o = simulation(subkey, theta)\n",
    "sl, g = jax.value_and_grad(surrogate_loss, argnums=1)(subkey, theta)\n",
    "\n",
    "print(f'Simulation outcome:\\t{o}')\n",
    "print(f'Surrogate loss value\\t{sl}')\n",
    "print(f'Gradient wrt theta:\\t{g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the true gradient we now have to get the expected value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average simulation outcome:\t0.8588588588588588\n",
      "Average surrogate_loss value\t0.30152299510446934\n",
      "Average gradient wrt theta:\t0.8015532928282839\n"
     ]
    }
   ],
   "source": [
    "theta = 1. # gives p = 0.73\n",
    "\n",
    "N_AVG = 1000\n",
    "\n",
    "key, *subkeys = jax.random.split(key, N_AVG)\n",
    "subkeys = np.asarray(subkeys)\n",
    "\n",
    "o = jax.vmap(simulation, in_axes=(0,None))(subkeys, theta)\n",
    "sl, g = jax.vmap(jax.value_and_grad(surrogate_loss, argnums=1), in_axes=(0,None))(subkeys, theta)\n",
    "\n",
    "\n",
    "print(f'Average simulation outcome:\\t{o.mean()}')\n",
    "print(f'Average surrogate_loss value\\t{sl.mean()}')\n",
    "print(f'Average gradient wrt theta:\\t{g.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** While the average simulation outcome is the same in both cases (as it should be), the estimated gradient is different! Straight-through estimators and score function estimators have different characteristcs and are subject to different tradeoffs that can be found in the literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Distributions\n",
    "\n",
    "The same ideas can be translated almost exactly to the case of a categorical distribution. A categorical distribution is a discrete distribution over a finite set of outcomes, each with a given probability. The probability mass function of a categorical distribution is given by:\n",
    "\n",
    "$$\n",
    "P(X = k) = \n",
    "\\begin{cases}\n",
    "p_k & \\text{if } k = 1, \\ldots, K \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $p_k$ is the probability of the $k$-th outcome.\n",
    "\n",
    "This is the case that is the most similar to the model presented in the paper, where each category $k$ is a single cell with its associated probability of division $p_k$.\n",
    "\n",
    "\n",
    "## A toy model (softmax growth)\n",
    "\n",
    "In order to showcase the application of the previous ideas to the more relevant (for us) case of categorical distributions, we present a toy model with very simple rules that we will optimize with gradient descent.\n",
    "\n",
    "The rules of the game are the following:\n",
    "- We have three categories of objects. The **system state** $\\bar x = (x_1, ..., x_N)$ is how many objects of each category we have at some point in time.\n",
    "- At each time step, one category is chosen and an object of the same category added to it. Objects of each category are characterized by a \"propensity\" of division $\\bar \\beta = (\\beta_1, ..., \\beta_N)$.\n",
    "- Category $k$ is chosen with probability \n",
    "$$ p_k = \\text{softmax}_k(\\bar \\beta, \\bar x) = \\frac{\\exp[-\\beta_k x_k]}{\\sum_i \\exp[-\\beta_i x_i]} $$\n",
    "- One game is composed of $T$ rounds.\n",
    "\n",
    "\n",
    "The aim of the optimization is to choose the propensities $\\bar \\beta$ in such a way that the system state at the end of the game is as close as possible to a target state $\\bar x^*$. We will define the loss function as the squared distance between the final state and the target state:\n",
    "\n",
    "$$ L_{\\bar \\beta}(\\bar x_T) = \\sum_i (x_i(T) - x_i^*)^2 $$\n",
    "\n",
    "Note that in this case the dependency on $\\bar \\beta$ of the loss function is given by the fact that \n",
    "$$ \\bar x(T) = \\mathbb F_{\\bar \\beta}[\\bar x(0)] $$\n",
    "\n",
    "where $\\mathbb F$ is the dynamics system state, which clearly depend on the chosen parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3 # number of categories\n",
    "\n",
    "T = 20 # number of time steps per simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target final state:\t[ 6  2 12]\n"
     ]
    }
   ],
   "source": [
    "# build target final state\n",
    "\n",
    "tx1 = T//3\n",
    "tx2 = T//7\n",
    "tx3 = T - tx1 - tx2\n",
    "target_x = np.array([tx1, tx2, tx3])\n",
    "\n",
    "print(f'Target final state:\\t{target_x}')\n",
    "\n",
    "\n",
    "# define square loss on state\n",
    "def loss_x(x, target_x):\n",
    "    return np.sum((x - target_x)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate probabilities\n",
    "p_fn = lambda x, betas: jax.nn.softmax(x * betas)\n",
    "\n",
    "# sample category index\n",
    "sample_category_idx = lambda subkey, p: jax.random.choice(subkey, len(p), p=p)\n",
    "\n",
    "# update system state\n",
    "update_state = lambda x, z: x + jax.nn.one_hot(z, len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation(subkey, x0, betas, T):\n",
    "\n",
    "    def _sim_step(subkey, x, betas):\n",
    "\n",
    "        p = p_fn(x, betas)\n",
    "        z = sample_category_idx(subkey, p)\n",
    "        x = update_state(x, z)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    subkeys = jax.random.split(subkey, T)\n",
    "\n",
    "    x = x0\n",
    "    for k in subkeys:\n",
    "        x = _sim_step(k, x, betas)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state:\t\t[1. 1. 1.]\n",
      "Final state:\t\t[19.  2.  2.]\n",
      "Target final state:\t[ 6  2 12]\n"
     ]
    }
   ],
   "source": [
    "### Carry out a trial simulation\n",
    "\n",
    "x0 = np.ones(N)\n",
    "betas = np.ones(N)\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "xT = simulation(subkey, x0, betas, T)\n",
    "\n",
    "print(f'Initial state:\\t\\t{x0}')\n",
    "print(f'Final state:\\t\\t{xT}')\n",
    "print(f'Target final state:\\t{target_x}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Straight-through Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
